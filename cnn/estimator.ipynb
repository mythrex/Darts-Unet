{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from model_search import Network\n",
    "from architect_graph import Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from genotypes import Genotype\n",
    "# tf.enable_eager_execution()\n",
    "tf.set_random_seed(6969)\n",
    "np.random.seed(6969)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"arch_learning_rate\": 3e-1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"grad_clip\": 5,\n",
    "    \"learning_rate\": 0.025,\n",
    "    \"learning_rate_decay\": 0.97,\n",
    "    \"learning_rate_min\": 0.0001,\n",
    "    \"num_batches_per_epoch\": 2000,\n",
    "    \n",
    "    \"unrolled\": True,\n",
    "    \"epochs\": 10,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"batch_size\": 8,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"save\": \"EXP\",\n",
    "    \"init_channels\": 3,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_classes\": 6,\n",
    "    \"crop_size\": [8, 8],\n",
    "    \"save_checkpoints_steps\": 100,\n",
    "    \"model_dir\": 'gs://unet-darts/train-search-ckptss',\n",
    "    \"max_steps\": 10000,\n",
    "    # NEW\n",
    "    \"steps_per_eval\": 2,\n",
    "    \"num_train_examples\": 16,\n",
    "    \"num_batches_per_epoch\": 2,\n",
    "    #\n",
    "    \n",
    "    \"use_tpu\": False,\n",
    "    \"use_host_call\": False,\n",
    "    \"tpu\": 'unet-darts',\n",
    "#     \"zone\": 'us-central1-f',\n",
    "#     \"project\": \"isro-nas\"\n",
    "    \"zone\": None,\n",
    "    \"project\": None\n",
    "}\n",
    "\n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Struct(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        image_dataset = tf.data.TFRecordDataset(filename)\n",
    "        W, H = 16, 16\n",
    "\n",
    "        # Create a dictionary describing the features.  \n",
    "        image_feature_description = {\n",
    "            'name': tf.FixedLenFeature([], tf.string),  \n",
    "            'label_encoded': tf.FixedLenFeature([], tf.string),\n",
    "            'encoded': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        def _parse_image_function(example_proto):\n",
    "            # Parse the input tf.Example proto using the dictionary above.\n",
    "            feature= tf.parse_single_example(example_proto, image_feature_description)\n",
    "            image= feature['encoded']\n",
    "            label = feature['label_encoded']\n",
    "            name = feature['name']\n",
    "\n",
    "            image = tf.image.decode_png(image, channels=3)\n",
    "            label = tf.image.decode_png(label, channels=3)\n",
    "\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image = tf.image.resize(image, (W, H))\n",
    "            label = tf.cast(label, tf.float32)\n",
    "            label = tf.image.resize(label, (W, H))\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        dataset = image_dataset.map(_parse_image_function)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn2(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn(params):\n",
    "        W, H = args.crop_size[0], args.crop_size[1]\n",
    "        NUM_IMAGES = 20\n",
    "        x_train = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "        y_train = np.random.randint(0, args.num_classes, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "        x_valid = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "        y_valid = np.random.randint(0, args.num_classes, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "\n",
    "        ds = (x_train, x_valid), (y_train, y_valid)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
    "        num_epochs = None # indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size, drop_remainder=True)\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneSaver(tf.estimator.SessionRunHook):\n",
    "    def __init__(self, genotype):\n",
    "        self.genotype = genotype\n",
    "    \n",
    "    def begin(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    def end(self, session):\n",
    "        normal_gene_op = self.genotype.normal\n",
    "        reduce_gene_op = self.genotype.reduce\n",
    "        \n",
    "        self.global_step = session.run(self.global_step)\n",
    "        normal_gene = session.run(normal_gene_op)\n",
    "        reduce_gene = session.run(reduce_gene_op)\n",
    "        \n",
    "        genotype = Genotype(\n",
    "            normal=normal_gene, normal_concat=self.genotype.normal_concat,\n",
    "            reduce=reduce_gene, reduce_concat=self.genotype.reduce_concat\n",
    "        )\n",
    "        \n",
    "        filename = 'final_genotype.{}'.format((self.global_step))\n",
    "        tf.logging.info(\"Saving Genotype for step: {}\".format(str(self.global_step)))\n",
    "        utils.write_genotype(genotype, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    criterion = tf.losses.softmax_cross_entropy\n",
    "    model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion, num_classes=args.num_classes)\n",
    "\n",
    "    global_step = tf.train.get_global_step()\n",
    "    learning_rate_min = tf.constant(args.learning_rate_min)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        args.learning_rate,\n",
    "        global_step,\n",
    "        decay_rate=args.learning_rate_decay,\n",
    "        decay_steps=args.num_batches_per_epoch,\n",
    "        staircase=True,\n",
    "    )\n",
    "    \n",
    "    lr = tf.maximum(learning_rate, learning_rate_min)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(lr, args.momentum)\n",
    "    criterion = tf.losses.sigmoid_cross_entropy\n",
    "    \n",
    "    eval_hooks = None\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    prediction_dict = None\n",
    "    export_outputs = None\n",
    "    # 2. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        (x_train, x_valid) = features\n",
    "        (y_train, y_valid) = labels\n",
    "\n",
    "        preds = model(x_train)\n",
    "        architect = Architect(model, args)\n",
    "        # architect step\n",
    "        # architect_step = architect.step(input_train=x_train,\n",
    "        #                                target_train=y_train,\n",
    "        #                                input_valid=x_valid,\n",
    "        #                                target_valid=y_valid,\n",
    "        #                                unrolled=args.unrolled,\n",
    "        #                                )\n",
    "\n",
    "\n",
    "        w_var = model.get_thetas()\n",
    "        loss = model._loss(preds, y_train)\n",
    "        grads = tf.gradients(loss, w_var)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, w_var), global_step=tf.train.get_global_step())\n",
    "\n",
    "#         miou = tf.metrics.mean_iou(\n",
    "#                     labels=y_train,\n",
    "#                     predictions=preds,\n",
    "#                     num_classes=args.num_classes\n",
    "#                 )\n",
    "#         acc = tf.metrics.accuracy(labels=y_train, \n",
    "#                                   predictions=preds)\n",
    "#         eval_metric_ops = {\n",
    "#             \"miou\": miou,\n",
    "#             \"accuracy\": acc\n",
    "#         }\n",
    "    \n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         global_step = tf.train.get_global_step()\n",
    "        genotype = model.genotype()\n",
    "        gene_saver = GeneSaver(genotype)\n",
    "        eval_hooks = [gene_saver]\n",
    "        \n",
    "        (x_train, x_valid) = features\n",
    "        (y_train, y_valid) = labels\n",
    "        preds = model(x_valid)\n",
    "        loss = model._loss(preds, y_valid)\n",
    "        miou = tf.metrics.mean_iou(\n",
    "                    labels=y_valid,\n",
    "                    predictions=preds,\n",
    "                    num_classes=args.num_classes\n",
    "                )\n",
    "        acc = tf.metrics.accuracy(labels=y_valid, predictions=preds)\n",
    "        eval_metric_ops = {\n",
    "            \"miou\": miou,\n",
    "            \"accuracy\": acc\n",
    "        }\n",
    "        \n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        x = features\n",
    "        y = labels\n",
    "        preds = model(x)\n",
    "        prediction_dict = {\"predictions\": preds}\n",
    "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = preds)}\n",
    "    \n",
    "    # 5. Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = prediction_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs,\n",
    "        evaluation_hooks=eval_hooks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to read in respective datasets\n",
    "def get_train():\n",
    "    return make_inp_fn2(filename = '../../datasets/infer/infer-00000-00007.tfrecords',\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      IMAGE_LOC: tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    \n",
    "    feature_placeholders['IMAGES'] = tf.placeholder(tf.float32, [None, args.crop_size[0], args.crop_size[1], args.init_channels])\n",
    "    \n",
    "    features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.estimator.RunConfig(save_checkpoints_steps=args.save_checkpoints_steps,\n",
    "                                model_dir=args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom estimator's train and evaluate function\n",
    "def train_and_evaluate(output_dir):\n",
    "    estimator = tf.estimator.Estimator(model_fn = model_fn, \n",
    "                         config=config)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = get_train(),\n",
    "                                    max_steps = 1000)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = get_train(),\n",
    "                                  steps = None, throttle_secs=600)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdf71d1b410>, '_model_dir': 'gs://unet-darts/train-search-ckptss', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from gs://unet-darts/train-search-ckptss/model.ckpt-990\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 990 into gs://unet-darts/train-search-ckptss/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.8951552, step = 991\n",
      "INFO:tensorflow:Saving checkpoints for 991 into gs://unet-darts/train-search-ckptss/model.ckpt.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "INFO:tensorflow:Loss for final step: 1.8951552.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x7fdf71d1b5d0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn = model_fn, \n",
    "                     config=config)\n",
    "estimator.train(input_fn = get_train(), steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-3.13719720e-01, -2.10635364e-01,  1.38333917e-01,\n",
       "           3.39387923e-01, -1.60747975e-01,  1.60995185e-01,\n",
       "          -1.68886140e-01,  9.87251550e-02, -1.93222836e-01],\n",
       "         [ 8.36660713e-02, -1.48222148e-01, -6.59976080e-02,\n",
       "           2.71289200e-01,  1.66718140e-02,  2.15990886e-01,\n",
       "           2.59747684e-01, -1.11556143e-01, -1.71548184e-02],\n",
       "         [-1.15309805e-01, -1.01198070e-01,  7.82530010e-02,\n",
       "           1.33930445e-01,  8.42399299e-02,  1.42105207e-01,\n",
       "           1.29673615e-01, -1.32413516e-02, -3.27356756e-02]],\n",
       "\n",
       "        [[-3.66295017e-02, -5.70783243e-02,  2.88080454e-01,\n",
       "           1.81272760e-01,  2.16118053e-01,  1.60589665e-01,\n",
       "           7.94534236e-02, -9.77155864e-02, -9.80683491e-02],\n",
       "         [ 5.20635694e-02, -2.41009116e-01,  2.20164493e-01,\n",
       "          -1.39509052e-01,  1.60952106e-01,  1.41411439e-01,\n",
       "          -1.96783587e-01, -2.34680533e-01, -1.19687259e-01],\n",
       "         [-1.46452323e-01,  1.94279477e-01,  2.51671493e-01,\n",
       "           3.20930779e-02, -1.73517004e-01,  2.19907463e-01,\n",
       "           1.02962498e-02,  7.38414973e-02, -9.40309241e-02]],\n",
       "\n",
       "        [[-2.63511389e-01, -1.60822257e-01, -1.28845856e-01,\n",
       "           1.31729394e-01, -1.16199248e-01, -1.82926312e-01,\n",
       "          -9.10445675e-02,  6.57942612e-03, -5.68359252e-03],\n",
       "         [ 1.30438790e-01,  1.37165129e-01, -5.51750734e-02,\n",
       "          -1.47027090e-01,  2.05318760e-02, -1.04180686e-01,\n",
       "           1.11294150e-01, -2.44237795e-01,  1.05901726e-01],\n",
       "         [ 1.93227753e-01, -2.59898424e-01,  2.56174266e-01,\n",
       "           3.66740435e-01, -1.96870342e-01, -2.32218787e-01,\n",
       "           1.02233671e-01, -7.35006705e-02, -2.54433602e-01]]],\n",
       "\n",
       "\n",
       "       [[[-5.28560095e-02,  1.95241570e-02, -3.05725262e-02,\n",
       "           1.36432335e-01, -3.85309133e-04, -7.34931603e-02,\n",
       "           1.63367428e-02,  1.10227235e-01, -2.36233696e-01],\n",
       "         [-2.52910256e-01,  9.94409099e-02, -1.02670163e-01,\n",
       "           5.81897199e-02,  2.04171866e-01,  2.30534300e-01,\n",
       "          -2.10401621e-02,  6.51559383e-02,  1.33715063e-01],\n",
       "         [-8.56702775e-02,  1.64929301e-01,  4.73872293e-03,\n",
       "           2.65686363e-01, -1.47578403e-01, -1.96274862e-01,\n",
       "          -3.71467434e-02,  1.66475568e-02, -1.50011051e-02]],\n",
       "\n",
       "        [[-3.55687946e-01,  1.28450558e-01,  4.53284867e-02,\n",
       "           2.83600837e-01,  1.70971744e-03, -1.06553137e-01,\n",
       "           1.28924213e-02, -1.31740525e-01, -2.29411080e-01],\n",
       "         [-1.47975400e-01, -1.30575985e-01,  2.28173003e-01,\n",
       "           1.69799253e-02, -1.42590031e-01,  1.51294515e-01,\n",
       "          -1.66364163e-01,  8.42286348e-02, -1.62783965e-01],\n",
       "         [-9.26421210e-02, -2.40376964e-01, -7.69320279e-02,\n",
       "           1.28853142e-01, -2.18079686e-01, -2.33179659e-01,\n",
       "          -1.87380657e-01,  1.40840814e-01, -2.88885534e-01]],\n",
       "\n",
       "        [[ 1.58502787e-01, -8.52671452e-03,  3.67933244e-01,\n",
       "           1.29809380e-01, -2.26539716e-01,  1.09432243e-01,\n",
       "          -2.87348665e-02, -6.88351691e-02,  3.42496894e-02],\n",
       "         [-8.57436955e-02,  8.95683318e-02,  4.16853398e-01,\n",
       "           7.24497736e-02,  1.49545908e-01,  6.08441010e-02,\n",
       "          -1.23374544e-01,  7.32070878e-02,  6.90701827e-02],\n",
       "         [-8.36430490e-02,  9.97829214e-02,  1.19720604e-02,\n",
       "          -1.33495927e-01, -3.26734073e-02,  2.52693176e-01,\n",
       "           1.77706465e-01, -6.94556162e-02, -8.96393359e-02]]],\n",
       "\n",
       "\n",
       "       [[[-1.22635819e-01,  5.12860753e-02,  8.08546394e-02,\n",
       "           3.25745672e-01, -1.95626259e-01, -5.01682833e-02,\n",
       "          -9.17164460e-02, -7.92651623e-02, -6.38538301e-02],\n",
       "         [-5.45319878e-02,  3.92708890e-02,  1.34746403e-01,\n",
       "           5.14604785e-02, -1.83580473e-01, -4.10760641e-02,\n",
       "          -1.86204746e-01,  1.93151966e-01,  1.26750857e-01],\n",
       "         [ 3.81531864e-02, -2.18361422e-01,  7.11627752e-02,\n",
       "           6.25043362e-02,  2.35277399e-01, -7.72380531e-02,\n",
       "           6.93357140e-02,  2.07963735e-01, -2.23495230e-01]],\n",
       "\n",
       "        [[-5.40507510e-02,  8.51406530e-02, -1.02597229e-01,\n",
       "           6.31899983e-02,  1.33447126e-01, -9.26585272e-02,\n",
       "          -7.44408071e-02, -2.67151028e-01, -2.76274756e-02],\n",
       "         [ 1.57241046e-01, -6.68754876e-02,  9.27968323e-02,\n",
       "           2.11156487e-01,  3.70114334e-02,  9.09073874e-02,\n",
       "           8.95560458e-02,  1.18917279e-01, -1.23894624e-01],\n",
       "         [ 7.08658844e-02, -1.17315553e-01,  3.68647814e-01,\n",
       "           2.06017215e-02, -1.84111819e-01,  1.85755193e-01,\n",
       "           2.07791459e-02, -2.61552006e-01, -2.73193419e-01]],\n",
       "\n",
       "        [[ 1.05828129e-01, -1.19015045e-01,  7.56357536e-02,\n",
       "           4.63069268e-02, -3.37242112e-02, -1.93216473e-01,\n",
       "           2.22164467e-01, -2.14987338e-01, -1.08401768e-01],\n",
       "         [-1.07915193e-01, -2.63370782e-01,  4.50793356e-02,\n",
       "          -1.24290153e-01, -1.61417663e-01,  9.07788575e-02,\n",
       "           2.14819923e-01,  1.24499209e-01, -6.91461042e-02],\n",
       "         [-3.75464521e-02, -1.44163162e-01,  3.24394405e-01,\n",
       "           3.53586972e-02,  1.93311259e-01, -9.19561982e-02,\n",
       "          -6.42081490e-03, -1.54036045e-01, -6.27943501e-03]]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.get_variable_value('network/sequential/conv2d/kernel:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbe35be9250>, '_model_dir': './outputdir', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From model_search.py:97: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = tf.losses.sigmoid_cross_entropy\n",
    "model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion, num_classes=args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-adce5ce8a041>:1: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_valid), (y_train, y_valid) = get_train()().make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:2' shape=(4, 4, 4, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "miou = tf.metrics.mean_iou(\n",
    "    labels=y_train,\n",
    "    predictions=logits,\n",
    "    num_classes=args.num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_init_op = tf.global_variables_initializer()\n",
    "local_init_op = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([global_init_op, local_init_op])\n",
    "    pred = sess.run(logits)\n",
    "    miou_out = sess.run(miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.],\n",
       "         [3.],\n",
       "         [0.],\n",
       "         [4.]],\n",
       "\n",
       "        [[5.],\n",
       "         [1.],\n",
       "         [3.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [4.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [3.],\n",
       "         [2.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [2.],\n",
       "         [2.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [4.],\n",
       "         [4.]]],\n",
       "\n",
       "\n",
       "       [[[2.],\n",
       "         [2.],\n",
       "         [5.],\n",
       "         [5.]],\n",
       "\n",
       "        [[4.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [5.]],\n",
       "\n",
       "        [[1.],\n",
       "         [2.],\n",
       "         [4.],\n",
       "         [0.]],\n",
       "\n",
       "        [[4.],\n",
       "         [5.],\n",
       "         [2.],\n",
       "         [1.]]],\n",
       "\n",
       "\n",
       "       [[[2.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [5.]],\n",
       "\n",
       "        [[2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [0.]],\n",
       "\n",
       "        [[5.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [3.]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([[ 0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 14.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  8.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  8.,  0.,  0.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miou_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
