{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from model_search import Network\n",
    "from architect_graph import Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from genotypes import Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"arch_learning_rate\": 3e-1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"grad_clip\": 5,\n",
    "    \"learning_rate\": 0.025,\n",
    "    \"learning_rate_decay\": 0.97,\n",
    "    \"learning_rate_min\": 0.0001,\n",
    "    \"num_batches_per_epoch\": 2000,\n",
    "    \n",
    "    \"unrolled\": True,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 4,\n",
    "    \"save\": \"EXP\",\n",
    "    \"init_channels\": 3,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_classes\": 6,\n",
    "    \"crop_size\": [4, 4],\n",
    "    \"save_checkpoints_steps\": 100,\n",
    "    \"model_dir\": './outputdir'\n",
    "}\n",
    "\n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Struct(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        image_dataset = tf.data.TFRecordDataset(filename)\n",
    "        W, H = 16, 16\n",
    "\n",
    "        # Create a dictionary describing the features.  \n",
    "        image_feature_description = {\n",
    "            'name': tf.FixedLenFeature([], tf.string),  \n",
    "            'label_encoded': tf.FixedLenFeature([], tf.string),\n",
    "            'encoded': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        def _parse_image_function(example_proto):\n",
    "            # Parse the input tf.Example proto using the dictionary above.\n",
    "            feature= tf.parse_single_example(example_proto, image_feature_description)\n",
    "            image= feature['encoded']\n",
    "            label = feature['label_encoded']\n",
    "            name = feature['name']\n",
    "\n",
    "            image = tf.image.decode_png(image, channels=3)\n",
    "            label = tf.image.decode_png(label, channels=3)\n",
    "\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image = tf.image.resize(image, (W, H))\n",
    "            label = tf.cast(label, tf.float32)\n",
    "            label = tf.image.resize(label, (W, H))\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        dataset = image_dataset.map(_parse_image_function)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn2(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        W, H = args.crop_size[0], args.crop_size[1]\n",
    "        NUM_IMAGES = 20\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "            x_train = np.random.randint(0, 256, (NUM_IMAGES // batch_size, batch_size, W, H, 3)).astype(np.float32)\n",
    "            y_train = np.random.randint(0, args.num_classes, (NUM_IMAGES // batch_size, batch_size, W, H, 1)).astype(np.float32)\n",
    "            x_valid = np.random.randint(0, 256, (NUM_IMAGES // batch_size, batch_size, W, H, 3)).astype(np.float32)\n",
    "            y_valid = np.random.randint(0, args.num_classes, (NUM_IMAGES // batch_size, batch_size, W, H, 1)).astype(np.float32)\n",
    "            \n",
    "            ds = (x_train, x_valid), (y_train, y_valid)\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            x_train = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "            y_train = np.random.randint(0, 6, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "            \n",
    "            ds = (x_train, y_train)\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "#         w, h, c = dataset.shape\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneSaver(tf.estimator.SessionRunHook):\n",
    "    def __init__(self, genotype):\n",
    "        self.genotype = genotype\n",
    "    \n",
    "    def begin(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    def end(self, session):\n",
    "        normal_gene_op = self.genotype.normal\n",
    "        reduce_gene_op = self.genotype.reduce\n",
    "        \n",
    "        self.global_step = session.run(self.global_step)\n",
    "        normal_gene = session.run(normal_gene_op)\n",
    "        reduce_gene = session.run(reduce_gene_op)\n",
    "        \n",
    "        genotype = Genotype(\n",
    "            normal=normal_gene, normal_concat=self.genotype.normal_concat,\n",
    "            reduce=reduce_gene, reduce_concat=self.genotype.reduce_concat\n",
    "        )\n",
    "        \n",
    "        filename = 'final_genotype.{}'.format((self.global_step))\n",
    "        tf.logging.info(\"Saving Genotype for step: {}\".format(str(self.global_step)))\n",
    "        utils.write_genotype(genotype, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    criterion = tf.losses.sigmoid_cross_entropy\n",
    "    model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion)\n",
    "    print(\"$$$$$$$$$$$$$$$$$\", tf.trainable_variables())\n",
    "    global_step = tf.train.get_global_step()\n",
    "    learning_rate_min = tf.constant(args.learning_rate_min)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        args.learning_rate,\n",
    "        global_step,\n",
    "        decay_rate=args.learning_rate_decay,\n",
    "        decay_steps=args.num_batches_per_epoch,\n",
    "        staircase=True,\n",
    "    )\n",
    "    \n",
    "    lr = tf.maximum(learning_rate, learning_rate_min)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(lr, args.momentum)\n",
    "    criterion = tf.losses.sigmoid_cross_entropy\n",
    "    \n",
    "    eval_hooks = None\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    prediction_dict = None\n",
    "    export_outputs = None\n",
    "    # 2. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        (x_train, x_valid) = features\n",
    "        (y_train, y_valid) = labels\n",
    "\n",
    "        preds = model(x_train)\n",
    "        architect = Architect(model, args)\n",
    "        # architect step\n",
    "        architect_step = architect.step(input_train=x_train,\n",
    "                                        target_train=y_train,\n",
    "                                        input_valid=x_valid,\n",
    "                                        target_valid=y_valid,\n",
    "                                        unrolled=args.unrolled,\n",
    "                                        )\n",
    "\n",
    "\n",
    "        w_var = model.get_thetas()\n",
    "        loss = model._loss(preds, y_train)\n",
    "        grads = tf.gradients(loss, w_var)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(grads, args.grad_clip)\n",
    "        train_op = optimizer.apply_gradients(zip(clipped_gradients, w_var), global_step=tf.train.get_global_step())\n",
    "\n",
    "        miou = tf.metrics.mean_iou(\n",
    "                    labels=y_train,\n",
    "                    predictions=preds,\n",
    "                    num_classes=args.num_classes\n",
    "                )\n",
    "        acc = tf.metrics.accuracy(labels=y_train, \n",
    "                                  predictions=preds)\n",
    "        eval_metric_ops = {\n",
    "            \"miou\": miou,\n",
    "            \"accuracy\": acc\n",
    "        }\n",
    "    \n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         global_step = tf.train.get_global_step()\n",
    "        genotype = model.genotype()\n",
    "        gene_saver = GeneSaver(genotype)\n",
    "        eval_hooks = [gene_saver]\n",
    "        \n",
    "        (x_train, x_valid) = features\n",
    "        (y_train, y_valid) = labels\n",
    "        preds = model(x_valid)\n",
    "        loss = model._loss(preds, y_valid)\n",
    "        miou = tf.metrics.mean_iou(\n",
    "                    labels=y_valid,\n",
    "                    predictions=preds,\n",
    "                    num_classes=args.num_classes\n",
    "                )\n",
    "        acc = tf.metrics.accuracy(labels=y_valid, predictions=preds)\n",
    "        eval_metric_ops = {\n",
    "            \"miou\": miou,\n",
    "            \"accuracy\": acc\n",
    "        }\n",
    "        \n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        x = features\n",
    "        y = labels\n",
    "        preds = model(x)\n",
    "        prediction_dict = {\"predictions\": preds}\n",
    "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = preds)}\n",
    "    \n",
    "    # 5. Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = prediction_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs,\n",
    "        evaluation_hooks=eval_hooks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to read in respective datasets\n",
    "def get_train():\n",
    "    return make_inp_fn2(filename = '../../datasets/infer/infer-00000-00007.tfrecords',\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      IMAGE_LOC: tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    \n",
    "    feature_placeholders['IMAGES'] = tf.placeholder(tf.float32, [None, args.crop_size[0], args.crop_size[1], args.init_channels])\n",
    "    \n",
    "    features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.estimator.RunConfig(save_checkpoints_steps=args.save_checkpoints_steps,\n",
    "                                model_dir=args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom estimator's train and evaluate function\n",
    "def train_and_evaluate(output_dir):\n",
    "    estimator = tf.estimator.Estimator(model_fn = model_fn, \n",
    "                         config=config)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = get_train(),\n",
    "                                    max_steps = 1000)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = get_train(),\n",
    "                                  steps = None, throttle_secs=600)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb49aa5eb90>, '_model_dir': './outputdir', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/training_util.py:236: initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "('$$$$$$$$$$$$$$$$$', [<tf.Variable 'alphas_normal:0' shape=(14, 2) dtype=float32_ref>, <tf.Variable 'alphas_reduce:0' shape=(14, 2) dtype=float32_ref>])\n",
      "WARNING:tensorflow:From architect_graph.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From architect_graph.py:105: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'network_1_1/sequential_1/conv2d_14/kernel:0' shape=(3, 3, 3, 9) dtype=float32>\", \"<tf.Variable 'network_1_1/sequential_1/batch_normalization_1/gamma:0' shape=(9,) dtype=float32>\", \"<tf.Variable 'network_1_1/sequential_1/batch_normalization_1/beta:0' shape=(9,) dtype=float32>\", \"<tf.Variable 'conv2d_11/kernel:0' shape=(1, 1, 9, 3) dtype=float32>\", \"<tf.Variable 'conv2d_12/kernel:0' shape=(1, 1, 9, 3) dtype=float32>\", \"<tf.Variable 'conv2d_13/kernel:0' shape=(1, 1, 9, 6) dtype=float32>\", \"<tf.Variable 'conv2d_14/kernel:0' shape=(1, 1, 12, 6) dtype=float32>\", \"<tf.Variable 'conv2d_15/kernel:0' shape=(1, 1, 12, 3) dtype=float32>\", \"<tf.Variable 'conv2d_16/kernel:0' shape=(1, 1, 12, 3) dtype=float32>\", \"<tf.Variable 'conv2d_17/kernel:0' shape=(1, 1, 24, 6) dtype=float32>\", \"<tf.Variable 'conv2d_18/kernel:0' shape=(1, 1, 24, 3) dtype=float32>\", \"<tf.Variable 'conv2d_19/kernel:0' shape=(1, 1, 24, 3) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_3/kernel:0' shape=(3, 3, 12, 3) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_3/bias:0' shape=(12,) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_4/kernel:0' shape=(3, 3, 3, 24) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_4/bias:0' shape=(3,) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_5/kernel:0' shape=(3, 3, 3, 24) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_5/bias:0' shape=(3,) dtype=float32>\", \"<tf.Variable 'conv2d_21/kernel:0' shape=(1, 1, 3, 3) dtype=float32>\", \"<tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 24, 3) dtype=float32>\", \"<tf.Variable 'network_1_1/softmaxConv/conv2d_27/kernel:0' shape=(1, 1, 12, 1) dtype=float32>\", \"<tf.Variable 'network_1_1/softmaxConv/conv2d_27/bias:0' shape=(1,) dtype=float32>\"] and loss Tensor(\"sigmoid_cross_entropy_loss_1/value:0\", shape=(), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-72ca637a0be1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m estimator = tf.estimator.Estimator(model_fn = model_fn, \n\u001b[1;32m      2\u001b[0m                      config=config)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m       \u001b[0msaving_listeners\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m       \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss for final step: %s.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1159\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_train_model_default\u001b[0;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[1;32m   1189\u001b[0m       \u001b[0mworker_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_hooks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m       estimator_spec = self._call_model_fn(\n\u001b[0;32m-> 1191\u001b[0;31m           features, labels, ModeKeys.TRAIN, self.config)\n\u001b[0m\u001b[1;32m   1192\u001b[0m       \u001b[0mglobal_step_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_global_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/estimator.pyc\u001b[0m in \u001b[0;36m_call_model_fn\u001b[0;34m(self, features, labels, mode, config)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m     \u001b[0mmodel_fn_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done calling model_fn.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-d386b802f30c>\u001b[0m in \u001b[0;36mmodel_fn\u001b[0;34m(features, labels, mode)\u001b[0m\n\u001b[1;32m     39\u001b[0m                                         \u001b[0minput_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                                         \u001b[0mtarget_valid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                                         \u001b[0munrolled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munrolled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                                         )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pankajb_sac_isro_gov_in/Darts-Unet/cnn/architect_graph.pyc\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, input_train, target_train, input_valid, target_valid, unrolled)\u001b[0m\n\u001b[1;32m     87\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_model_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                                                \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m                                               )\n\u001b[1;32m     91\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/pankajb_sac_isro_gov_in/Darts-Unet/cnn/architect_graph.pyc\u001b[0m in \u001b[0;36m_compute_unrolled_step\u001b[0;34m(self, x_train, y_train, x_valid, y_valid, w_var, train_loss, lr)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy_weight_opts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0munrolled_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientDescentOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0munrolled_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munrolled_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munrolled_train_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munrolled_w_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mvalid_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munrolled_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/optimizer.pyc\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    408\u001b[0m           \u001b[0;34m\"No gradients provided for any variable, check your graph for ops\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m           \u001b[0;34m\" that do not support gradients, between variables %s and loss %s.\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m           ([str(v) for _, v in grads_and_vars], loss))\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     return self.apply_gradients(grads_and_vars, global_step=global_step,\n",
      "\u001b[0;31mValueError\u001b[0m: No gradients provided for any variable, check your graph for ops that do not support gradients, between variables [\"<tf.Variable 'network_1_1/sequential_1/conv2d_14/kernel:0' shape=(3, 3, 3, 9) dtype=float32>\", \"<tf.Variable 'network_1_1/sequential_1/batch_normalization_1/gamma:0' shape=(9,) dtype=float32>\", \"<tf.Variable 'network_1_1/sequential_1/batch_normalization_1/beta:0' shape=(9,) dtype=float32>\", \"<tf.Variable 'conv2d_11/kernel:0' shape=(1, 1, 9, 3) dtype=float32>\", \"<tf.Variable 'conv2d_12/kernel:0' shape=(1, 1, 9, 3) dtype=float32>\", \"<tf.Variable 'conv2d_13/kernel:0' shape=(1, 1, 9, 6) dtype=float32>\", \"<tf.Variable 'conv2d_14/kernel:0' shape=(1, 1, 12, 6) dtype=float32>\", \"<tf.Variable 'conv2d_15/kernel:0' shape=(1, 1, 12, 3) dtype=float32>\", \"<tf.Variable 'conv2d_16/kernel:0' shape=(1, 1, 12, 3) dtype=float32>\", \"<tf.Variable 'conv2d_17/kernel:0' shape=(1, 1, 24, 6) dtype=float32>\", \"<tf.Variable 'conv2d_18/kernel:0' shape=(1, 1, 24, 3) dtype=float32>\", \"<tf.Variable 'conv2d_19/kernel:0' shape=(1, 1, 24, 3) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_3/kernel:0' shape=(3, 3, 12, 3) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_3/bias:0' shape=(12,) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_4/kernel:0' shape=(3, 3, 3, 24) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_4/bias:0' shape=(3,) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_5/kernel:0' shape=(3, 3, 3, 24) dtype=float32>\", \"<tf.Variable 'conv2d_transpose_5/bias:0' shape=(3,) dtype=float32>\", \"<tf.Variable 'conv2d_21/kernel:0' shape=(1, 1, 3, 3) dtype=float32>\", \"<tf.Variable 'conv2d_20/kernel:0' shape=(3, 3, 24, 3) dtype=float32>\", \"<tf.Variable 'network_1_1/softmaxConv/conv2d_27/kernel:0' shape=(1, 1, 12, 1) dtype=float32>\", \"<tf.Variable 'network_1_1/softmaxConv/conv2d_27/bias:0' shape=(1,) dtype=float32>\"] and loss Tensor(\"sigmoid_cross_entropy_loss_1/value:0\", shape=(), dtype=float32)."
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn = model_fn, \n",
    "                     config=config)\n",
    "estimator.train(input_fn = get_train(), steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbe35be9250>, '_model_dir': './outputdir', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From model_search.py:97: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = tf.losses.sigmoid_cross_entropy\n",
    "model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion, num_classes=args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-adce5ce8a041>:1: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_valid), (y_train, y_valid) = get_train()().make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:2' shape=(4, 4, 4, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "miou = tf.metrics.mean_iou(\n",
    "    labels=y_train,\n",
    "    predictions=logits,\n",
    "    num_classes=args.num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_init_op = tf.global_variables_initializer()\n",
    "local_init_op = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([global_init_op, local_init_op])\n",
    "    pred = sess.run(logits)\n",
    "    miou_out = sess.run(miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.],\n",
       "         [3.],\n",
       "         [0.],\n",
       "         [4.]],\n",
       "\n",
       "        [[5.],\n",
       "         [1.],\n",
       "         [3.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [4.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [3.],\n",
       "         [2.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [2.],\n",
       "         [2.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [4.],\n",
       "         [4.]]],\n",
       "\n",
       "\n",
       "       [[[2.],\n",
       "         [2.],\n",
       "         [5.],\n",
       "         [5.]],\n",
       "\n",
       "        [[4.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [5.]],\n",
       "\n",
       "        [[1.],\n",
       "         [2.],\n",
       "         [4.],\n",
       "         [0.]],\n",
       "\n",
       "        [[4.],\n",
       "         [5.],\n",
       "         [2.],\n",
       "         [1.]]],\n",
       "\n",
       "\n",
       "       [[[2.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [5.]],\n",
       "\n",
       "        [[2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [0.]],\n",
       "\n",
       "        [[5.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [3.]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([[ 0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 14.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  8.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  8.,  0.,  0.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miou_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
