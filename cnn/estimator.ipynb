{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from model_search import Network\n",
    "from architect_graph import Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from genotypes import Genotype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"arch_learning_rate\": 3e-1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"grad_clip\": 5,\n",
    "    \"learning_rate\": 0.025,\n",
    "    \"learning_rate_decay\": 0.97,\n",
    "    \"learning_rate_min\": 0.0001,\n",
    "    \"num_batches_per_epoch\": 2000,\n",
    "    \n",
    "    \"unrolled\": True,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 4,\n",
    "    \"save\": \"EXP\",\n",
    "    \"init_channels\": 3,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_classes\": 6,\n",
    "    \"crop_size\": [4, 4],\n",
    "    \"save_checkpoints_steps\": 100,\n",
    "    \"model_dir\": './outputdir'\n",
    "}\n",
    "\n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Struct(**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        image_dataset = tf.data.TFRecordDataset(filename)\n",
    "        W, H = 16, 16\n",
    "\n",
    "        # Create a dictionary describing the features.  \n",
    "        image_feature_description = {\n",
    "            'name': tf.FixedLenFeature([], tf.string),  \n",
    "            'label_encoded': tf.FixedLenFeature([], tf.string),\n",
    "            'encoded': tf.FixedLenFeature([], tf.string)\n",
    "        }\n",
    "        def _parse_image_function(example_proto):\n",
    "            # Parse the input tf.Example proto using the dictionary above.\n",
    "            feature= tf.parse_single_example(example_proto, image_feature_description)\n",
    "            image= feature['encoded']\n",
    "            label = feature['label_encoded']\n",
    "            name = feature['name']\n",
    "\n",
    "            image = tf.image.decode_png(image, channels=3)\n",
    "            label = tf.image.decode_png(label, channels=3)\n",
    "\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            image = tf.image.resize(image, (W, H))\n",
    "            label = tf.cast(label, tf.float32)\n",
    "            label = tf.image.resize(label, (W, H))\n",
    "\n",
    "            return image, label\n",
    "\n",
    "        dataset = image_dataset.map(_parse_image_function)\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn2(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn():\n",
    "        W, H = args.crop_size[0], args.crop_size[1]\n",
    "        NUM_IMAGES = 20\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "            x_train = np.random.randint(0, 256, (NUM_IMAGES // batch_size, batch_size, W, H, 3)).astype(np.float32)\n",
    "            y_train = np.random.randint(0, args.num_classes, (NUM_IMAGES // batch_size, batch_size, W, H, 1)).astype(np.float32)\n",
    "            x_valid = np.random.randint(0, 256, (NUM_IMAGES // batch_size, batch_size, W, H, 3)).astype(np.float32)\n",
    "            y_valid = np.random.randint(0, args.num_classes, (NUM_IMAGES // batch_size, batch_size, W, H, 1)).astype(np.float32)\n",
    "            \n",
    "            ds = (x_train, x_valid), (y_train, y_valid)\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
    "            num_epochs = None # indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "        else:\n",
    "            x_train = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "            y_train = np.random.randint(0, 6, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "            \n",
    "            ds = (x_train, y_train)\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
    "            num_epochs = 1 # end-of-input after this\n",
    "#         w, h, c = dataset.shape\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneSaver(tf.estimator.SessionRunHook):\n",
    "    def __init__(self, genotype):\n",
    "        self.genotype = genotype\n",
    "    \n",
    "    def begin(self):\n",
    "        self.global_step = tf.train.get_or_create_global_step()\n",
    "    \n",
    "    def end(self, session):\n",
    "        normal_gene_op = self.genotype.normal\n",
    "        reduce_gene_op = self.genotype.reduce\n",
    "        \n",
    "        self.global_step = session.run(self.global_step)\n",
    "        normal_gene = session.run(normal_gene_op)\n",
    "        reduce_gene = session.run(reduce_gene_op)\n",
    "        \n",
    "        genotype = Genotype(\n",
    "            normal=normal_gene, normal_concat=self.genotype.normal_concat,\n",
    "            reduce=reduce_gene, reduce_concat=self.genotype.reduce_concat\n",
    "        )\n",
    "        \n",
    "        filename = 'final_genotype.{}'.format((self.global_step))\n",
    "        tf.logging.info(\"Saving Genotype for step: {}\".format(str(self.global_step)))\n",
    "        utils.write_genotype(genotype, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    criterion = tf.losses.sigmoid_cross_entropy\n",
    "    model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion)\n",
    "    global_step = tf.train.get_global_step()\n",
    "    learning_rate_min = tf.constant(args.learning_rate_min)\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        args.learning_rate,\n",
    "        global_step,\n",
    "        decay_rate=args.learning_rate_decay,\n",
    "        decay_steps=args.num_batches_per_epoch,\n",
    "        staircase=True,\n",
    "    )\n",
    "    \n",
    "    lr = tf.maximum(learning_rate, learning_rate_min)\n",
    "    tf.summary.scalar('learning_rate', lr)\n",
    "    \n",
    "    optimizer = tf.train.MomentumOptimizer(lr, args.momentum)\n",
    "    criterion = tf.losses.sigmoid_cross_entropy\n",
    "    \n",
    "    eval_hooks = None\n",
    "    \n",
    "    loss = None\n",
    "    train_op = None\n",
    "    eval_metric_ops = None\n",
    "    prediction_dict = None\n",
    "    export_outputs = None\n",
    "    # 2. Loss function, training/eval ops\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        (x_train, x_valid) = features\n",
    "        (y_train, y_valid) = labels\n",
    "\n",
    "        preds = model(x_train)\n",
    "        architect = Architect(model, args)\n",
    "        # architect step\n",
    "        architect_step = architect.step(input_train=x_train,\n",
    "                                        target_train=y_train,\n",
    "                                        input_valid=x_valid,\n",
    "                                        target_valid=y_valid,\n",
    "                                        unrolled=args.unrolled,\n",
    "                                        )\n",
    "\n",
    "\n",
    "        w_var = model.get_thetas()\n",
    "        loss = model._loss(preds, y_train)\n",
    "        grads = tf.gradients(loss, w_var)\n",
    "        clipped_gradients, norm = tf.clip_by_global_norm(grads, args.grad_clip)\n",
    "        train_op = optimizer.apply_gradients(zip(clipped_gradients, w_var), global_step=tf.train.get_global_step())\n",
    "\n",
    "        miou = tf.metrics.mean_iou(\n",
    "                    labels=y_train,\n",
    "                    predictions=preds,\n",
    "                    num_classes=args.num_classes\n",
    "                )\n",
    "        acc = tf.metrics.accuracy(labels=y_train, \n",
    "                                  predictions=preds)\n",
    "        eval_metric_ops = {\n",
    "            \"miou\": miou,\n",
    "            \"accuracy\": acc\n",
    "        }\n",
    "    \n",
    "    elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "#         global_step = tf.train.get_global_step()\n",
    "        genotype = model.genotype()\n",
    "        gene_saver = GeneSaver(genotype)\n",
    "        eval_hooks = [gene_saver]\n",
    "        \n",
    "        (x_train, x_valid) = features\n",
    "        (y_train, y_valid) = labels\n",
    "        preds = model(x_valid)\n",
    "        loss = model._loss(preds, y_valid)\n",
    "        miou = tf.metrics.mean_iou(\n",
    "                    labels=y_valid,\n",
    "                    predictions=preds,\n",
    "                    num_classes=args.num_classes\n",
    "                )\n",
    "        acc = tf.metrics.accuracy(labels=y_valid, predictions=preds)\n",
    "        eval_metric_ops = {\n",
    "            \"miou\": miou,\n",
    "            \"accuracy\": acc\n",
    "        }\n",
    "        \n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        x = features\n",
    "        y = labels\n",
    "        preds = model(x)\n",
    "        prediction_dict = {\"predictions\": preds}\n",
    "        export_outputs = {\"predict_export_outputs\": tf.estimator.export.PredictOutput(outputs = preds)}\n",
    "    \n",
    "    # 5. Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = prediction_dict,\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = export_outputs,\n",
    "        evaluation_hooks=eval_hooks\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions to read in respective datasets\n",
    "def get_train():\n",
    "    return make_inp_fn2(filename = '../../datasets/infer/infer-00000-00007.tfrecords',\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create serving input function\n",
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "      IMAGE_LOC: tf.placeholder(tf.float32, [None])\n",
    "    }\n",
    "    \n",
    "    feature_placeholders['IMAGES'] = tf.placeholder(tf.float32, [None, args.crop_size[0], args.crop_size[1], args.init_channels])\n",
    "    \n",
    "    features = {\n",
    "    key: tf.expand_dims(tensor, -1)\n",
    "    for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.estimator.RunConfig(save_checkpoints_steps=args.save_checkpoints_steps,\n",
    "                                model_dir=args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create custom estimator's train and evaluate function\n",
    "def train_and_evaluate(output_dir):\n",
    "    estimator = tf.estimator.Estimator(model_fn = model_fn, \n",
    "                         config=config)\n",
    "    train_spec = tf.estimator.TrainSpec(input_fn = get_train(),\n",
    "                                    max_steps = 1000)\n",
    "    exporter = tf.estimator.LatestExporter('exporter', serving_input_fn)\n",
    "    eval_spec = tf.estimator.EvalSpec(input_fn = get_train(),\n",
    "                                  steps = None, throttle_secs=600)\n",
    "    tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb124087650>, '_model_dir': './outputdir', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Could not find trained model in model_dir: ./outputdir, running initialization to evaluate.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From model_search.py:97: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:Entity <bound method Softmax.call of <operations.Softmax object at 0x7fb1236467d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Softmax.call of <operations.Softmax object at 0x7fb1236467d0>>, which Python reported as:\n",
      "    def call(self, x):\n",
      "#         return self.op(x, axis)\n",
      "        return x\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING: Entity <bound method Softmax.call of <operations.Softmax object at 0x7fb1236467d0>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Softmax.call of <operations.Softmax object at 0x7fb1236467d0>>, which Python reported as:\n",
      "    def call(self, x):\n",
      "#         return self.op(x, axis)\n",
      "        return x\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:From model_search.py:198: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-11-15T06:11:54Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Saving Genotype for step: 0\n",
      "INFO:tensorflow:Finished evaluation at 2019-11-15-06:11:56\n",
      "INFO:tensorflow:Saving dict for global step 0: accuracy = 0.0, global_step = 0, loss = 3.9059103, miou = 0.0390625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0, 'global_step': 0, 'loss': 3.9059103, 'miou': 0.0390625}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator = tf.estimator.Estimator(model_fn = model_fn, \n",
    "                     config=config)\n",
    "estimator.evaluate(input_fn = get_train(), steps = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fbe35be9250>, '_model_dir': './outputdir', '_protocol': None, '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 100, '_device_fn': None, '_session_creation_timeout_secs': 7200, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_experimental_max_worker_delay_secs': None, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps 100 or save_checkpoints_secs None.\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From model_search.py:97: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = tf.losses.sigmoid_cross_entropy\n",
    "model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion, num_classes=args.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-adce5ce8a041>:1: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "(x_train, x_valid), (y_train, y_valid) = get_train()().make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "logits = model(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'IteratorGetNext:2' shape=(4, 4, 4, 1) dtype=float32>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/metrics_impl.py:1178: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "miou = tf.metrics.mean_iou(\n",
    "    labels=y_train,\n",
    "    predictions=logits,\n",
    "    num_classes=args.num_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_init_op = tf.global_variables_initializer()\n",
    "local_init_op = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([global_init_op, local_init_op])\n",
    "    pred = sess.run(logits)\n",
    "    miou_out = sess.run(miou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]],\n",
       "\n",
       "\n",
       "       [[[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]],\n",
       "\n",
       "        [[3.],\n",
       "         [3.],\n",
       "         [3.],\n",
       "         [3.]]]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[3.],\n",
       "         [3.],\n",
       "         [0.],\n",
       "         [4.]],\n",
       "\n",
       "        [[5.],\n",
       "         [1.],\n",
       "         [3.],\n",
       "         [1.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [1.],\n",
       "         [4.]],\n",
       "\n",
       "        [[0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.]]],\n",
       "\n",
       "\n",
       "       [[[0.],\n",
       "         [3.],\n",
       "         [2.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [2.],\n",
       "         [2.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]],\n",
       "\n",
       "        [[0.],\n",
       "         [0.],\n",
       "         [4.],\n",
       "         [4.]]],\n",
       "\n",
       "\n",
       "       [[[2.],\n",
       "         [2.],\n",
       "         [5.],\n",
       "         [5.]],\n",
       "\n",
       "        [[4.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [5.]],\n",
       "\n",
       "        [[1.],\n",
       "         [2.],\n",
       "         [4.],\n",
       "         [0.]],\n",
       "\n",
       "        [[4.],\n",
       "         [5.],\n",
       "         [2.],\n",
       "         [1.]]],\n",
       "\n",
       "\n",
       "       [[[2.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [5.]],\n",
       "\n",
       "        [[2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [0.]],\n",
       "\n",
       "        [[1.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [0.]],\n",
       "\n",
       "        [[5.],\n",
       "         [2.],\n",
       "         [2.],\n",
       "         [3.]]]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.Session().run(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, array([[ 0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 14.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  8.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 12.,  0.,  0.],\n",
       "        [ 0.,  0.,  0., 11.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  8.,  0.,  0.]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "miou_out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
