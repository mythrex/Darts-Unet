{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YLD9kfyhk_kX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZLKwgbKYrJ9D"
   },
   "outputs": [],
   "source": [
    "from architect import Architect\n",
    "from model_search import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(\"cifar\")\n",
    "    parser.add_argument('--data', type=str, default='./data',\n",
    "                        help='location of the data corpus')\n",
    "    parser.add_argument('--batch_size', type=int,\n",
    "                        default=1, help='batch size')\n",
    "    parser.add_argument('--learning_rate', type=float,\n",
    "                        default=0.025, help='init learning rate')\n",
    "    parser.add_argument('--learning_rate_min', type=float,\n",
    "                        default=0.001, help='min learning rate')\n",
    "    parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "    parser.add_argument('--weight_decay', type=float,\n",
    "                        default=3e-4, help='weight decay')\n",
    "    parser.add_argument('--report_freq', type=float,\n",
    "                        default=10, help='report frequency')\n",
    "    parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "    parser.add_argument('--epochs', type=int, default=50,\n",
    "                        help='num of training epochs')\n",
    "    parser.add_argument('--init_channels', type=int,\n",
    "                        default=3, help='num of init channels')\n",
    "    parser.add_argument('--layers', type=int, default=5,\n",
    "                        help='total number of layers')\n",
    "    parser.add_argument('--model_path', type=str,\n",
    "                        default='saved_models', help='path to save the model')\n",
    "    parser.add_argument('--cutout', action='store_true',\n",
    "                        default=False, help='use cutout')\n",
    "    parser.add_argument('--cutout_length', type=int,\n",
    "                        default=16, help='cutout length')\n",
    "    parser.add_argument('--drop_path_prob', type=float,\n",
    "                        default=0.3, help='drop path probability')\n",
    "    parser.add_argument('--save', type=str, default='EXP',\n",
    "                        help='experiment name')\n",
    "    parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "    parser.add_argument('--grad_clip', type=float,\n",
    "                        default=5, help='gradient clipping')\n",
    "    parser.add_argument('--train_portion', type=float,\n",
    "                        default=1.0, help='portion of training data')\n",
    "    parser.add_argument('--unrolled', action='store_true',\n",
    "                        default=False, help='use one-step unrolled validation loss')\n",
    "    parser.add_argument('--arch_learning_rate', type=float,\n",
    "                        default=3e-4, help='learning rate for arch encoding')\n",
    "    parser.add_argument('--arch_weight_decay', type=float,\n",
    "                        default=1e-3, help='weight decay for arch encoding')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zJ1PyV08lBtE"
   },
   "source": [
    "# Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DZnKAbg0lDud"
   },
   "outputs": [],
   "source": [
    "def train(x_train, y_train, x_valid, y_valid, model, architect, optimizer):\n",
    "    \"\"\"Trains the network. Gradient step is performed here\n",
    "\n",
    "    Args:\n",
    "        train_queue (array): Train queue\n",
    "        valid_queue (array): Validation queue\n",
    "        model (Network): Network\n",
    "        architect (Architect): the architechture of network\n",
    "        criterion (fn): Loss function\n",
    "        optimizer (Optimiser): Adam / SGD\n",
    "        lr (float): Learning Rate\n",
    "\n",
    "    Returns:\n",
    "        (float, float): returns acc and miOu\n",
    "    \"\"\"\n",
    "\n",
    "    # architect step\n",
    "    architect_step = architect.step(input_train=x_train,\n",
    "                                    target_train=y_train,\n",
    "                                    input_valid=x_valid,\n",
    "                                    target_valid=y_valid,\n",
    "                                    unrolled=args.unrolled,\n",
    "                                    )\n",
    "    \n",
    "    with tf.control_dependencies([architect_step]):\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_train)\n",
    "            w_var = model.get_thetas()\n",
    "            loss = model._criterion(logits, y_train)\n",
    "            grads = tape.gradient(loss, w_var)\n",
    "            clipped_gradients, norm = tf.clip_by_global_norm(grads, args.grad_clip)\n",
    "            opt_op = optimizer.apply_gradients(zip(clipped_gradients, w_var))\n",
    "\n",
    "    # calculating accuracy and iou\n",
    "    acc = utils.accuracy(logits, y_train)\n",
    "    iou = utils.iou(logits, y_train)\n",
    "\n",
    "    return loss, acc, iou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6YjEkiLOWnjm"
   },
   "source": [
    "## Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vqCIe6kipVvk"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"arch_learning_rate\": 3e-1,\n",
    "    \"arch_weight_decay\": 1e-3,\n",
    "    \"momentum\": 0.9,\n",
    "    \"grad_clip\": 5,\n",
    "    \"learning_rate_min\": 0.001,\n",
    "    \"learning_rate\": 0.025,\n",
    "    \"unrolled\": True,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 4,\n",
    "    \"save\": \"EXP\"\n",
    "}\n",
    "\n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Struct(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sy8PC0OllMG9"
   },
   "outputs": [],
   "source": [
    "np_ds_train = (np.random.randint(0, 256, (20, 16, 16, 3)).astype(np.float32), np.random.randint(0, 2, (20, 16, 16, 1)).astype(np.float32))\n",
    "np_ds_valid = (np.random.randint(0, 256, (20, 16, 16, 3)).astype(np.float32), np.random.randint(0, 2, (20, 16, 16, 1)).astype(np.float32))\n",
    "ds_train = tf.data.Dataset.from_tensor_slices(np_ds_train).batch(4)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices(np_ds_valid).batch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3CPLYkYlNty"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0926 14:12:10.865862 139687464281920 deprecation_wrapper.py:119] From /home/mythrex/Coding/Darts-Unet/cnn/operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "W0926 14:12:10.873477 139687464281920 deprecation_wrapper.py:119] From /home/mythrex/Coding/Darts-Unet/cnn/operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "W0926 14:12:10.921126 139687464281920 deprecation_wrapper.py:119] From /home/mythrex/Coding/Darts-Unet/cnn/model_search.py:97: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = tf.losses.sigmoid_cross_entropy\n",
    "model = Network(3, 3, criterion)\n",
    "optimizer = tf.train.MomentumOptimizer(args.learning_rate_min, args.momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqiLkcVjra3x"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0926 14:12:12.999419 139687464281920 deprecation_wrapper.py:119] From /home/mythrex/Coding/Darts-Unet/cnn/architect.py:39: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "architect = Architect(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1gpUdoDXlRjl"
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 604
    },
    "colab_type": "code",
    "id": "CFSOA_mo4Z_1",
    "outputId": "7000648e-2ec1-4a6f-afe9-f733b3718857"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\u001b[A\u001b[A\u001b[A\n"
     ]
    }
   ],
   "source": [
    "for e in range(2):\n",
    "    train_it = ds_train.make_one_shot_iterator()\n",
    "    valid_it = ds_valid.make_one_shot_iterator()\n",
    "    for i in tqdm(range(1)):\n",
    "        x_train, y_train = train_it.get_next()\n",
    "        x_valid, y_valid = valid_it.get_next()\n",
    "        \n",
    "        loss, acc_op, iou_op = train(x_train,\n",
    "                                   y_train,\n",
    "                                   x_valid,\n",
    "                                   y_valid,\n",
    "                                   model,\n",
    "                                   architect,\n",
    "                                   optimizer\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RIYsbuKgC4Gi"
   },
   "source": [
    "# Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LlyRQ9al643O"
   },
   "outputs": [],
   "source": [
    "def infer(x_valid, y_valid, logits, model, criterion):\n",
    "    loss_op = model._loss(logits, y_valid)\n",
    "    acc_op = utils.accuracy(logits, y_valid)\n",
    "    iou_op = utils.iou(logits, y_valid)\n",
    "    return loss_op, acc_op, iou_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rJSvp5FQrLg"
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMryBc1UQr2u"
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    np_ds_train = (np.random.randint(0, 256, (20, 16, 16, 3)).astype(np.float32), np.random.randint(0, 2, (20, 16, 16, 1)).astype(np.float32))\n",
    "    np_ds_valid = (np.random.randint(0, 256, (20, 16, 16, 3)).astype(np.float32), np.random.randint(0, 2, (20, 16, 16, 1)).astype(np.float32))\n",
    "    ds_train = tf.data.Dataset.from_tensor_slices(np_ds_train).batch(args.batch_size)\n",
    "    ds_valid = tf.data.Dataset.from_tensor_slices(np_ds_valid).batch(args.batch_size)\n",
    "\n",
    "    num_iterations = int(np_ds_train[0].shape[0] / args.batch_size)\n",
    "\n",
    "    criterion = tf.losses.sigmoid_cross_entropy\n",
    "    model = Network(3, 3, criterion)\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.MomentumOptimizer(args.learning_rate_min, args.momentum)\n",
    "\n",
    "    architect = Architect(model, args)\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    _logits = model(tf.convert_to_tensor(np_ds_train[0][:4]))\n",
    "    mious = []\n",
    "    for e in range(args.epochs):\n",
    "        tf.logging.info('Epoch {}'.format(e))\n",
    "\n",
    "        train_it = ds_train.make_one_shot_iterator()\n",
    "        valid_it = ds_valid.make_one_shot_iterator()\n",
    "\n",
    "        tq1 = tqdm(range(num_iterations))\n",
    "        genotype = model.genotype()\n",
    "\n",
    "        # Train Loop\n",
    "        train_miou = 0\n",
    "        train_unions, train_intersections = [], []    \n",
    "\n",
    "        for i in tq1:\n",
    "            x_train, y_train = train_it.get_next()\n",
    "            x_valid, y_valid = valid_it.get_next()\n",
    "            train_loss, train_acc, train_iou = train(x_train=x_train,\n",
    "                                                     y_train=y_train,\n",
    "                                                     x_valid=x_valid,\n",
    "                                                     y_valid=y_valid,\n",
    "                                                     model=model,\n",
    "                                                     architect=architect,\n",
    "                                                     optimizer=optimizer\n",
    "                                                     )\n",
    "            if(i % args.report_freq == 0):\n",
    "                tq1.set_postfix({\n",
    "                        \"Train Loss\": train_loss.numpy(),\n",
    "                        \"Train Acc\": train_acc.numpy(),\n",
    "                        \"Train IoU\": train_iou[0].numpy()\n",
    "                        })\n",
    "            train_intersections.append(train_iou[1].numpy())\n",
    "            train_unions.append(train_iou[2].numpy())\n",
    "    \n",
    "        # Calculation of train miou\n",
    "        train_unions = np.array(train_unions)\n",
    "        train_intersections = np.array(train_intersections)\n",
    "        train_non_zero_mask = train_unions != 0\n",
    "        train_miou = np.mean(train_intersections[train_non_zero_mask])/(np.mean(train_unions[train_non_zero_mask]) + 1e-6)\n",
    "\n",
    "        # Log train miou\n",
    "        logging.info('Train mIoU: {}'.format(train_miou))\n",
    "\n",
    "        # Validation loop\n",
    "        valid_unions, valid_intersections = [], []\n",
    "        tq2 = tqdm(range(num_iterations))\n",
    "        for i in tq2:\n",
    "            valid_logits = model(x_valid)\n",
    "            valid_loss, valid_acc, valid_iou = infer(x_valid=x_valid, \n",
    "                                                       y_valid=y_valid,\n",
    "                                                       logits=valid_logits,\n",
    "                                                       model=model,\n",
    "                                                       criterion=criterion\n",
    "                                                       )\n",
    "\n",
    "            if(i % args.report_freq == 0):\n",
    "                tq2.set_postfix({\n",
    "                                \"Valid Loss\": valid_loss.numpy(),\n",
    "                                \"Valid Acc\": valid_acc.numpy(),\n",
    "                                \"Valid IoU\": valid_iou[0].numpy()\n",
    "                                })\n",
    "            valid_intersections.append(valid_iou[1])  \n",
    "            valid_unions.append(valid_iou[2])\n",
    "\n",
    "        # Calculation of train miou\n",
    "        valid_unions = np.array(valid_unions)\n",
    "        valid_intersections = np.array(valid_intersections)\n",
    "        valid_non_zero_mask = valid_unions != 0\n",
    "        valid_miou = np.mean(valid_intersections[valid_non_zero_mask])/(np.mean(valid_unions[valid_non_zero_mask]) + 1e-6)\n",
    "\n",
    "        #Log Miou\n",
    "        logging.info('Validation mIoU: {}'.format(valid_miou))\n",
    "\n",
    "        # save the final genotype\n",
    "        if(max(mIoUs) < valid_iou):\n",
    "            logging.info(\"Writing the computed genotype tp ./cnn/final_models/final_genotype.py\")\n",
    "            utils.write_genotype(genotype)\n",
    "\n",
    "        mious.append(valid_miou)\n",
    "  \n",
    "    np.save(os.path.join(args.save, \"mIoUs.npy\"), mIoUs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dDB4CtAam4Y"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdacmI61aj-d"
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    args = {\n",
    "        \"momentum\": 0.9,\n",
    "        \"weight_decay\": 3e-4,\n",
    "        \"arch_learning_rate\": 3e-1,\n",
    "        \"arch_weight_decay\": 1e-3,\n",
    "        \"momentum\": 0.9,\n",
    "        \"grad_clip\": 5,\n",
    "        \"learning_rate_min\": 0.001,\n",
    "        \"learning_rate\": 0.025,\n",
    "        \"unrolled\": True,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 4,\n",
    "        \"save\": \"EXP\",\n",
    "        \"report_freq\": 1\n",
    "    }\n",
    "\n",
    "    class Struct:\n",
    "        def __init__(self, **entries):\n",
    "            self.__dict__.update(entries)\n",
    "\n",
    "    return Struct(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3bvzQeHoaaUv",
    "outputId": "1a6babba-847a-4903-e7ea-a26548122380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20190926-180242\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    args = parse_args()\n",
    "    args.save = 'search-{}-{}'.format(args.save,\n",
    "                                      time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "    utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "    \n",
    "    # logging\n",
    "    log_format = '%(asctime)s %(message)s'\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "                        format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "    fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "    fh.setFormatter(logging.Formatter(log_format))\n",
    "    logging.getLogger().addHandler(fh)\n",
    "    \n",
    "\n",
    "#     main(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "6YjEkiLOWnjm"
   ],
   "name": "train_search",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
