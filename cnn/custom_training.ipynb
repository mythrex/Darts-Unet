{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import logging\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utils\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from model_search import Network\n",
    "from architect_graph import Architect\n",
    "import utils\n",
    "from genotypes import Genotype\n",
    "# tf.enable_eager_execution()\n",
    "tf.set_random_seed(6969)\n",
    "np.random.seed(6969)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"arch_learning_rate\": 3e-1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"grad_clip\": 5,\n",
    "    \"learning_rate\": 0.025,\n",
    "    \"learning_rate_decay\": 0.97,\n",
    "    \"learning_rate_min\": 0.0001,\n",
    "    \"num_batches_per_epoch\": 2000,\n",
    "    \n",
    "    \"unrolled\": True,\n",
    "    \"epochs\": 10,\n",
    "    \"train_batch_size\": 8,\n",
    "    \"batch_size\": 8,\n",
    "    \"eval_batch_size\": 8,\n",
    "    \"save\": \"EXP\",\n",
    "    \"init_channels\": 3,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_classes\": 6,\n",
    "    \"crop_size\": [8, 8],\n",
    "    \"save_checkpoints_steps\": 100,\n",
    "    \"model_dir\": 'gs://unet-darts/train-search-ckpts',\n",
    "#     \"max_steps\": 10000,\n",
    "    \"max_steps\": None,\n",
    "    \"steps\": 20,\n",
    "    # NEW\n",
    "    \"steps_per_eval\": 2,\n",
    "    \"num_train_examples\": 16,\n",
    "    \"num_batches_per_epoch\": 2,\n",
    "    #\n",
    "    \n",
    "    \"train_report_freq\": 10,\n",
    "    \n",
    "    \"use_tpu\": False,\n",
    "    \"use_host_call\": False,\n",
    "    \"tpu\": 'unet-darts',\n",
    "#     \"zone\": 'us-central1-f',\n",
    "#     \"project\": \"isro-nas\"\n",
    "    \"zone\": None,\n",
    "    \"project\": None\n",
    "}\n",
    "\n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Struct(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inp_fn2(filename, mode, batch_size):\n",
    "    \n",
    "    def _input_fn(params):\n",
    "        W, H = args.crop_size[0], args.crop_size[1]\n",
    "        NUM_IMAGES = 20\n",
    "        x_train = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "        y_train = np.random.randint(0, args.num_classes, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "        x_valid = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "        y_valid = np.random.randint(0, args.num_classes, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "\n",
    "        ds = (x_train, x_valid), (y_train, y_valid)\n",
    "        dataset = tf.data.Dataset.from_tensor_slices(ds)\n",
    "        num_epochs = None # indefinitely\n",
    "        dataset = dataset.shuffle(buffer_size = 10 * batch_size)\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size, drop_remainder=True)\n",
    "        return dataset\n",
    "    \n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_fn, args):\n",
    "    ds = input_fn(args)\n",
    "    it = ds.make_one_shot_iterator()\n",
    "    \n",
    "    criterion = tf.losses.softmax_cross_entropy\n",
    "    model = Network(C=args.init_channels, net_layers=args.num_layers, criterion=criterion, num_classes=args.num_classes)\n",
    "    global_step = tf.Variable(0, name='global_step',trainable=False)\n",
    "    learning_rate_min = tf.constant(args.learning_rate_min)\n",
    "\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        args.learning_rate,\n",
    "        global_step,\n",
    "        decay_rate=args.learning_rate_decay,\n",
    "        decay_steps=args.num_batches_per_epoch,\n",
    "        staircase=True,\n",
    "    )\n",
    "\n",
    "    lr = tf.maximum(learning_rate, learning_rate_min)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(lr, args.momentum)\n",
    "\n",
    "    features, labels = it.get_next()\n",
    "    (x_train, x_valid) = features\n",
    "    (y_train, y_valid) = labels\n",
    "\n",
    "    preds = model(x_train)\n",
    "    architect = Architect(model, args)\n",
    "    tf.logging.info('Arch Step')\n",
    "    architect_step = architect.step(input_train=x_train,\n",
    "                                    target_train=y_train,\n",
    "                                    input_valid=x_valid,\n",
    "                                    target_valid=y_valid,\n",
    "                                    unrolled=args.unrolled,\n",
    "                                    )\n",
    "    with tf.control_dependencies([architect_step]):\n",
    "        w_var = model.get_thetas()\n",
    "        loss = model._loss(preds, y_train)\n",
    "        grads = tf.gradients(loss, w_var)\n",
    "        tf.logging.info('Model Step')\n",
    "        train_op = optimizer.apply_gradients(zip(grads, w_var), global_step=tf.train.get_global_step())\n",
    "    \n",
    "    global_init_op = tf.global_variables_initializer()\n",
    "    tf.logging.info('Initialized Global Initializer.')\n",
    "    local_init_op = tf.local_variables_initializer()\n",
    "    tf.logging.info('Initialized Local Initializer.')\n",
    "    \n",
    "    current_step = tf.train.latest_checkpoint(args.model_dir)\n",
    "    next_checkpoint = 0\n",
    "    if(not current_step):\n",
    "        current_step = 0\n",
    "    else:\n",
    "        current_step = int(current_step.split('-')[-1])\n",
    "        \n",
    "    if(args.max_steps == None and args.steps):\n",
    "        next_checkpoint = current_step + args.steps\n",
    "    else:\n",
    "        next_checkpoint = args.max_steps\n",
    "    \n",
    "    tf.logging.info('Training for %d steps. Current step %d.' % (next_checkpoint, current_step))\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    tf.logging.info('Initialized Training Saver!')\n",
    "                    \n",
    "    with tf.Session() as sess:\n",
    "        latest_ckpt = tf.train.latest_checkpoint(args.model_dir)\n",
    "        tf.logging.info(\"Loading from %s\" % latest_ckpt)\n",
    "        saver.restore(sess, latest_ckpt)\n",
    "        \n",
    "        while(current_step <= next_checkpoint):            \n",
    "            sess.run([global_init_op, local_init_op])\n",
    "            w_var_o = sess.run(w_var)\n",
    "            sess.run(train_op)\n",
    "            w_var_o2 = sess.run(w_var)\n",
    "            \n",
    "            if(not current_step % args.train_report_freq):\n",
    "                tf.logging.info(\"Current Step: %d\" % current_step)\n",
    "            current_step += 1\n",
    "        save_path = saver.save(sess, os.path.join(args.model_dir, 'model.ckpt'))\n",
    "        tf.logging.info(\"Model saved in path: %s\" % save_path)\n",
    "    return w_var_o, w_var_o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/data/util/random_seed.py:58: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From <ipython-input-8-47b947cd7af6>:3: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n",
      "WARNING:tensorflow:From operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:6: The name tf.layers.AveragePooling2D is deprecated. Please use tf.compat.v1.layers.AveragePooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From model_search.py:93: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From architect_graph.py:41: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From architect_graph.py:47: The name tf.train.get_or_create_global_step is deprecated. Please use tf.compat.v1.train.get_or_create_global_step instead.\n",
      "\n",
      "INFO:tensorflow:Arch Step\n",
      "WARNING:tensorflow:From architect_graph.py:112: The name tf.train.GradientDescentOptimizer is deprecated. Please use tf.compat.v1.train.GradientDescentOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From architect_graph.py:124: The name tf.global_norm is deprecated. Please use tf.linalg.global_norm instead.\n",
      "\n",
      "INFO:tensorflow:Model Step\n",
      "INFO:tensorflow:Initialized Global Initializer.\n",
      "INFO:tensorflow:Initialized Local Initializer.\n",
      "INFO:tensorflow:Training for 20 steps. Current step 0.\n",
      "INFO:tensorflow:Initialized Training Saver!\n",
      "INFO:tensorflow:Loading from None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't load save_path when it is None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ab74174ecde0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m train(make_inp_fn2(filename = tf.gfile.Glob('../../datasets/train_search/train-search-*-00008.tfrecords'),\n\u001b[1;32m      2\u001b[0m                         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m                         batch_size = args.train_batch_size), args)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-47b947cd7af6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_fn, args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mlatest_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading from %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlatest_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_ckpt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mwhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_step\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mnext_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.pyc\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can't load save_path when it is None.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0mcheckpoint_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Can't load save_path when it is None."
     ]
    }
   ],
   "source": [
    "train(make_inp_fn2(filename = tf.gfile.Glob('../../datasets/train_search/train-search-*-00008.tfrecords'),\n",
    "                        mode = tf.estimator.ModeKeys.TRAIN,\n",
    "                        batch_size = args.train_batch_size), args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
