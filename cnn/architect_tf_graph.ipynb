{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _concat(xs):\n",
    "    \"\"\"nd tensor to 1d tensor\n",
    "\n",
    "    Args:\n",
    "        xs (array): the array of nd tensor\n",
    "\n",
    "    Returns:\n",
    "        array: concated array\n",
    "    \"\"\"\n",
    "    return tf.concat([tf.reshape(x, [tf.size(x)]) for x in xs], axis=0, name=\"_concat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[[[1],[2],[3]], [[4], [5], [6]]], [[[2],[4],[6]], [[8], [10], [12]]]])\n",
    "b = tf.constant([1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'concat/concat:0' shape=(12,) dtype=int32>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat(tf.reshape(a, [tf.size(a)]), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architect(object):\n",
    "    \"\"\"Constructs the model\n",
    "\n",
    "    Parameters:\n",
    "      network_momentum(float):  network momentum\n",
    "      network_weight_decay(float): network weight decay\n",
    "      model(Network): Network archtecture with cells\n",
    "      optimise(optimiser): Adam / SGD\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, args):\n",
    "        \"\"\"Initialises the architecture\n",
    "\n",
    "        Args:\n",
    "            model (Network): Network archtecture with cells\n",
    "            args (dict): cli args\n",
    "        \"\"\"\n",
    "        self.network_momentum = args.momentum\n",
    "        self.network_weight_decay = args.weight_decay\n",
    "        self.model = model\n",
    "        self.use_tpu = args.use_tpu\n",
    "        \n",
    "        self.arch_learning_rate = args.arch_learning_rate\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.arch_learning_rate,\n",
    "                                                beta1=0.5,\n",
    "                                                beta2=0.999)\n",
    "        if(self.use_tpu):\n",
    "            self.optimizer = tf.tpu.CrossShardOptimizer(self.optimizer)\n",
    "            \n",
    "        global_step = tf.train.get_or_create_global_step()\n",
    "        learning_rate_min = tf.constant(args.learning_rate_min)\n",
    "\n",
    "        learning_rate = tf.train.exponential_decay(\n",
    "            args.learning_rate,\n",
    "            global_step,\n",
    "            decay_rate=args.learning_rate_decay,\n",
    "            decay_steps=args.num_batches_per_epoch,\n",
    "            staircase=True,\n",
    "        )\n",
    "\n",
    "        lr = tf.maximum(learning_rate, learning_rate_min)\n",
    "        \n",
    "        self.learning_rate = lr\n",
    "\n",
    "    def get_model_theta(self, model):\n",
    "        specific_tensor = []\n",
    "        specific_tensor_name = []\n",
    "        for var in model.trainable_weights:\n",
    "            if not 'alphas' in var.name:\n",
    "                specific_tensor.append(var)\n",
    "                specific_tensor_name.append(var.name)\n",
    "        return specific_tensor\n",
    "    \n",
    "    def step(self, input_train, target_train, input_valid, target_valid, unrolled):\n",
    "        \"\"\"Computer a step for gradient descend\n",
    "\n",
    "        Args:\n",
    "            input_train (tensor): a train of input\n",
    "            target_train (tensor): a train of targets\n",
    "            input_valid (tensor): a train of validation\n",
    "            target_valid (tensor): a train of validation targets\n",
    "            eta (tensor): eta\n",
    "            network_optimizer (optimiser): network optimiser for network\n",
    "            unrolled (bool): True if training we need unrolled\n",
    "        \"\"\"\n",
    "        if unrolled:\n",
    "            w_regularization_loss = 0.25\n",
    "            logits = self.model(input_train)\n",
    "            train_loss = self.model._loss(logits, target_train)\n",
    "            train_loss += 1e4*0.25*w_regularization_loss\n",
    "            return self._compute_unrolled_step(input_train, \n",
    "                                               target_train, \n",
    "                                               input_valid, \n",
    "                                               target_valid,\n",
    "                                               self.get_model_theta(self.model),\n",
    "                                               train_loss,\n",
    "                                               self.learning_rate\n",
    "                                              )\n",
    "        else:\n",
    "            return self._backward_step(input_valid, target_valid)\n",
    "        \n",
    "    \n",
    "    def _compute_unrolled_step(self, x_train, y_train, x_valid, y_valid, w_var, train_loss, lr):\n",
    "        arch_var = self.model.arch_parameters()\n",
    "        \n",
    "        unrolled_model = self.model.new()\n",
    "        _ = unrolled_model(x_train)\n",
    "        unrolled_w_var = self.get_model_theta(unrolled_model)\n",
    "        copy_weight_opts = [v.assign(w) for v,w in zip(unrolled_w_var, w_var)]\n",
    "        logits = unrolled_model(x_train)\n",
    "        \n",
    "        unrolled_train_loss = unrolled_model._loss(logits, y_train)  \n",
    "\n",
    "        with tf.control_dependencies(copy_weight_opts):\n",
    "            unrolled_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "            if(self.use_tpu):\n",
    "                unrolled_optimizer = tf.tpu.CrossShardOptimizer(unrolled_optimizer)\n",
    "            unrolled_optimizer = unrolled_optimizer.minimize(unrolled_train_loss, var_list=unrolled_w_var)\n",
    "\n",
    "        valid_logits = unrolled_model(x_valid)\n",
    "        valid_loss = unrolled_model._loss(valid_logits, y_valid)\n",
    "\n",
    "        with tf.control_dependencies([unrolled_optimizer]):\n",
    "            valid_grads = tf.gradients(valid_loss, unrolled_w_var)\n",
    "\n",
    "        r=1e-2\n",
    "        R = r / (tf.global_norm(valid_grads)+1e-6)\n",
    "\n",
    "        optimizer_pos=tf.train.GradientDescentOptimizer(R)\n",
    "        if(self.use_tpu):\n",
    "            optimizer_pos = tf.tpu.CrossShardOptimizer(optimizer_pos)\n",
    "        optimizer_pos=optimizer_pos.apply_gradients(zip(valid_grads, w_var))\n",
    "\n",
    "        optimizer_neg=tf.train.GradientDescentOptimizer(-2*R)\n",
    "        if(self.use_tpu):\n",
    "            optimizer_neg = tf.tpu.CrossShardOptimizer(optimizer_neg)\n",
    "        optimizer_neg=optimizer_neg.apply_gradients(zip(valid_grads, w_var))\n",
    "\n",
    "        optimizer_back=tf.train.GradientDescentOptimizer(R)\n",
    "        if(self.use_tpu):\n",
    "            optimizer_back = tf.tpu.CrossShardOptimizer(optimizer_back)\n",
    "        optimizer_back=optimizer_back.apply_gradients(zip(valid_grads, w_var))\n",
    "        \n",
    "        with tf.control_dependencies([optimizer_pos]):\n",
    "            train_grads_pos=tf.gradients(train_loss, arch_var)\n",
    "            with tf.control_dependencies([optimizer_neg]):\n",
    "                train_grads_neg=tf.gradients(train_loss,arch_var)\n",
    "                with tf.control_dependencies([optimizer_back]):\n",
    "                    leader_opt= self.optimizer\n",
    "                    leader_grads=tf.gradients(valid_loss, unrolled_model.arch_parameters())\n",
    "        \n",
    "        for i,g in enumerate(leader_grads):\n",
    "            leader_grads[i]= g - self.learning_rate * tf.divide(train_grads_pos[i]-train_grads_neg[i],2*R)\n",
    "\n",
    "        leader_opt=leader_opt.apply_gradients(zip(leader_grads, arch_var))\n",
    "        return leader_opt\n",
    "    \n",
    "    def _backward_step(self, input_valid, target_valid):\n",
    "        \"\"\"Backward step for validation\n",
    "\n",
    "        Args:\n",
    "            input_train (tensor): a train of input\n",
    "            target_train (tensor): a train of targets\n",
    "        \"\"\"\n",
    "        loss = self.model._loss(self.model(input_valid), target_valid)\n",
    "        opt = self.optimizer.minimize(loss, var_list=model.get_weights())\n",
    "        return opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_search import Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From operations.py:290: The name tf.layers.Conv2D is deprecated. Please use tf.compat.v1.layers.Conv2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:7: The name tf.layers.MaxPooling2D is deprecated. Please use tf.compat.v1.layers.MaxPooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From operations.py:6: The name tf.layers.AveragePooling2D is deprecated. Please use tf.compat.v1.layers.AveragePooling2D instead.\n",
      "\n",
      "WARNING:tensorflow:From model_search.py:93: The name tf.layers.Conv2DTranspose is deprecated. Please use tf.compat.v1.layers.Conv2DTranspose instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = tf.losses.sigmoid_cross_entropy\n",
    "model = Network(3, 3, criterion, num_classes=)\n",
    "args = {\n",
    "    \"momentum\": 0.9,\n",
    "    \"weight_decay\": 3e-4,\n",
    "    \"arch_learning_rate\": 3e-1,\n",
    "    \"momentum\": 0.9,\n",
    "    \"grad_clip\": 5,\n",
    "    \"learning_rate\": 0.025,\n",
    "    \"learning_rate_decay\": 0.97,\n",
    "    \"learning_rate_min\": 0.0001,\n",
    "    \"num_batches_per_epoch\": 2000,\n",
    "    \n",
    "    \"unrolled\": True,\n",
    "    \"epochs\": 10,\n",
    "    \"train_batch_size\": 2,\n",
    "    \"eval_batch_size\": 2,\n",
    "    \"save\": \"EXP\",\n",
    "    \"init_channels\": 3,\n",
    "    \"num_layers\": 3,\n",
    "    \"num_classes\": 6,\n",
    "    \"crop_size\": [8, 8],\n",
    "    \"save_checkpoints_steps\": 100,\n",
    "    \"model_dir\": 'gs://unet-darts/train-search-ckptss',\n",
    "    \"max_steps\": 10000,\n",
    "    # NEW\n",
    "    \"steps_per_eval\": 2,\n",
    "    \"num_train_examples\": 16,\n",
    "    #\n",
    "    \n",
    "    \"use_tpu\": False,\n",
    "    \"use_host_call\": True,\n",
    "    \"tpu\": 'unet-darts',\n",
    "    \"zone\": 'us-central1-f',\n",
    "    \"project\": \"isro-nas\",\n",
    "}\n",
    "args.update({\"num_batches_per_epoch\": args[\"num_train_examples\"] // args[\"train_batch_size\"]})\n",
    "\n",
    "class Struct:\n",
    "    def __init__(self, **entries):\n",
    "        self.__dict__.update(entries)\n",
    "\n",
    "args = Struct(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-7e55eb507963>:11: make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `for ... in dataset:` to iterate over a dataset. If using `tf.estimator`, return the `Dataset` object directly from your input function. As a last resort, you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)`.\n"
     ]
    }
   ],
   "source": [
    "W, H = args.crop_size[0], args.crop_size[1]\n",
    "NUM_IMAGES = 20\n",
    "x_train = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "y_train = np.random.randint(0, args.num_classes, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "x_valid = np.random.randint(0, 256, (NUM_IMAGES, W, H, 3)).astype(np.float32)\n",
    "y_valid = np.random.randint(0, args.num_classes, (NUM_IMAGES, W, H, 1)).astype(np.float32)\n",
    "\n",
    "ds_train = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(args.train_batch_size, drop_remainder=True)\n",
    "ds_valid = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(args.train_batch_size, drop_remainder=True)\n",
    "\n",
    "it_train = ds_train.make_one_shot_iterator()\n",
    "image, label = it_train.get_next()\n",
    "it_valid = ds_valid.make_one_shot_iterator()\n",
    "image_valid, label_valid = it_valid.get_next()\n",
    "lr=0.025\n",
    "unrolled=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_estimator/python/estimator/api/_v1/estimator/__init__.py:12: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "res = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "architect = Architect(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "opt = architect.step(image, label, image_valid, label_valid, unrolled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf.initialize_all_variables().run()\n",
    "    out1 = sess.run(model.arch_parameters())\n",
    "    out = sess.run(opt)\n",
    "    out2 = sess.run(model.arch_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_theta(model):\n",
    "    specific_tensor = []\n",
    "    specific_tensor_name = []\n",
    "    for var in model.trainable_weights:\n",
    "        if not 'alphas' in var.name:\n",
    "            specific_tensor.append(var)\n",
    "            specific_tensor_name.append(var.name)\n",
    "    return specific_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "arch_var = model.arch_parameters()\n",
    "unrolled_model = model.new()\n",
    "_ = unrolled_model(image)\n",
    "unrolled_w_var = get_model_theta(unrolled_model)\n",
    "w_var = get_model_theta(model)\n",
    "copy_weight_opts = [v.assign(w) for v,w in zip(unrolled_w_var, w_var)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.control_dependencies(copy_weight_opts):\n",
    "    logits = unrolled_model(image)\n",
    "    unrolled_train_loss = unrolled_model._loss(logits, label)\n",
    "    unrolled_w_var = get_model_theta(unrolled_model)\n",
    "#     unrolled_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "#     unrolled_train_grads = unrolled_optimizer.compute_gradients(unrolled_train_loss, var_list=unrolled_w_var)\n",
    "#     unrolled_optimizer_op = unrolled_optimizer.apply_gradients(unrolled_train_grads)\n",
    "    unrolled_train_grads = tf.gradients(unrolled_train_loss, unrolled_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_logits = unrolled_model(image_valid)\n",
    "valid_loss = unrolled_model._loss(valid_logits, label_valid)\n",
    "valid_grads = tf.gradients(valid_loss, unrolled_w_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r=1e-2\n",
    "# R = r / (tf.global_norm(valid_grads)+1e-6)\n",
    "\n",
    "# optimizer_pos=tf.train.GradientDescentOptimizer(R)\n",
    "# optimizer_pos=optimizer_pos.apply_gradients(zip(valid_grads, w_var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    tf.initialize_all_variables().run()\n",
    "    out1 = sess.run(unrolled_train_grads)\n",
    "    out2 = sess.run(valid_grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[ 0.3001952 , -0.29948696,  0.3008377 ,  0.29996917],\n",
       "        [ 0.30031642, -0.2996615 , -0.2997647 ,  0.30070767],\n",
       "        [-0.2991369 ,  0.30045164,  0.3000093 , -0.2999449 ],\n",
       "        [ 0.30068585, -0.29991263, -0.29915914,  0.30049714],\n",
       "        [ 0.29826996,  0.29994002,  0.29957512, -0.29978627],\n",
       "        [ 0.30017585, -0.2998143 , -0.29921976,  0.3006019 ],\n",
       "        [ 0.30059466, -0.29950923,  0.30016547,  0.30075297],\n",
       "        [ 0.30009156, -0.29971334, -0.29942992,  0.3000586 ],\n",
       "        [ 0.3004881 , -0.29953355, -0.29924586,  0.30093056],\n",
       "        [ 0.29992858,  0.30010808, -0.29989988,  0.30026817],\n",
       "        [-0.29938245,  0.3005266 ,  0.3005949 , -0.2993926 ],\n",
       "        [-0.29937303,  0.300259  ,  0.30051953, -0.29907057],\n",
       "        [-0.29914486,  0.30060256,  0.30067992, -0.2994711 ],\n",
       "        [-0.29946443,  0.30038452,  0.3004314 , -0.2999285 ]],\n",
       "       dtype=float32),\n",
       " array([[ 0.30047026,  0.30066213, -0.2997903 ,  0.300108  ],\n",
       "        [ 0.3003905 , -0.29819813, -0.29910982,  0.2998683 ],\n",
       "        [ 0.30049688, -0.29895297, -0.29964027,  0.300947  ],\n",
       "        [ 0.29983646, -0.29813954, -0.29936862,  0.30037805],\n",
       "        [ 0.30033353, -0.2988572 , -0.29919812,  0.2998204 ],\n",
       "        [ 0.29565623,  0.3000595 , -0.29964155,  0.29921043],\n",
       "        [ 0.29980475, -0.2990774 , -0.2992466 ,  0.30008328],\n",
       "        [ 0.29941538, -0.2977185 , -0.29863575,  0.30022243],\n",
       "        [-0.2970534 , -0.29882294, -0.2990961 ,  0.30054292],\n",
       "        [ 0.29787043,  0.29914638, -0.29908392,  0.2997245 ],\n",
       "        [ 0.30013382, -0.29851726, -0.2993199 ,  0.3002416 ],\n",
       "        [ 0.30039626, -0.2903175 , -0.2988991 , -0.29275638],\n",
       "        [ 0.2993671 ,  0.291512  , -0.2992022 ,  0.29815164],\n",
       "        [-0.29918164, -0.2992075 , -0.2997466 ,  0.30072865]],\n",
       "       dtype=float32)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_normal = tf.get_variable(\"alphas_normal\", [14, 4])\n",
    "alphas_reduce = tf.get_variable(\"alphas_reduce\", [14, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_normal_t = tf.get_variable('alphas_normal', initializer=alphas_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../train-search-ckpts/model.ckpt-100\n",
      "[[7.6811708e-04 6.1610749e-04 7.3774339e-04 1.7251099e-04]\n",
      " [4.6491768e-04 1.1831308e-04 4.8414184e-04 8.2456821e-04]\n",
      " [3.8573553e-04 9.6499722e-04 4.1154864e-05 4.5638240e-04]\n",
      " [2.1470130e-04 8.5145357e-04 1.4969934e-04 6.1814004e-04]\n",
      " [7.2059088e-04 9.4683509e-04 9.7304612e-04 2.3769797e-04]\n",
      " [9.2476612e-06 7.6371187e-04 3.9916387e-04 6.2424364e-04]\n",
      " [4.5342889e-04 8.0606272e-04 4.9657130e-04 7.5611676e-04]\n",
      " [2.9467285e-04 9.4560866e-04 2.8576973e-04 2.9266250e-04]\n",
      " [1.9598247e-05 7.2383991e-04 7.3719054e-04 1.7768217e-04]\n",
      " [3.8440814e-04 8.4802596e-04 4.9985957e-04 5.6202081e-04]\n",
      " [4.0616884e-04 8.8108651e-04 2.6535750e-05 9.6354849e-04]\n",
      " [5.6696753e-04 1.9432831e-04 6.9372944e-04 2.0856393e-04]\n",
      " [5.4914324e-04 6.7978277e-04 6.0854066e-04 4.3868009e-04]\n",
      " [5.6436576e-04 2.2776485e-05 9.7615091e-04 5.3994311e-04]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "#     print(sess.run(alphas_normal))\n",
    "    saver.restore(sess, \"../../train-search-ckpts/model.ckpt-100\")\n",
    "    print(sess.run(alphas_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../train-search-ckpts/model.ckpt-200\n",
      "[[7.6811708e-04 6.1610749e-04 7.3774339e-04 1.7251099e-04]\n",
      " [4.6491768e-04 1.1831308e-04 4.8414184e-04 8.2456821e-04]\n",
      " [3.8573553e-04 9.6499722e-04 4.1154864e-05 4.5638240e-04]\n",
      " [2.1470130e-04 8.5145357e-04 1.4969934e-04 6.1814004e-04]\n",
      " [7.2059088e-04 9.4683509e-04 9.7304612e-04 2.3769797e-04]\n",
      " [9.2476612e-06 7.6371187e-04 3.9916387e-04 6.2424364e-04]\n",
      " [4.5342889e-04 8.0606272e-04 4.9657130e-04 7.5611676e-04]\n",
      " [2.9467285e-04 9.4560866e-04 2.8576973e-04 2.9266250e-04]\n",
      " [1.9598247e-05 7.2383991e-04 7.3719054e-04 1.7768217e-04]\n",
      " [3.8440814e-04 8.4802596e-04 4.9985957e-04 5.6202081e-04]\n",
      " [4.0616884e-04 8.8108651e-04 2.6535750e-05 9.6354849e-04]\n",
      " [5.6696753e-04 1.9432831e-04 6.9372944e-04 2.0856393e-04]\n",
      " [5.4914324e-04 6.7978277e-04 6.0854066e-04 4.3868009e-04]\n",
      " [5.6436576e-04 2.2776485e-05 9.7615091e-04 5.3994311e-04]]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"../../train-search-ckpts/model.ckpt-200\")\n",
    "    print(sess.run(alphas_normal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'alphas_reduce_1:0' shape=(14, 4) dtype=float32_ref>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphas_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sess' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1b3d0aa2ea7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAINABLE_VARIABLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sess' is not defined"
     ]
    }
   ],
   "source": [
    "variables = sess.graph.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
