{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Architect(object):\n",
    "    \"\"\"Constructs the model\n",
    "\n",
    "    Parameters:\n",
    "      network_momentum(float):  network momentum\n",
    "      network_weight_decay(float): network weight decay\n",
    "      model(Network): Network archtecture with cells\n",
    "      optimise(optimiser): Adam / SGD\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, args):\n",
    "        \"\"\"Initialises the architecture\n",
    "\n",
    "        Args:\n",
    "            model (Network): Network archtecture with cells\n",
    "            args (dict): cli args\n",
    "        \"\"\"\n",
    "        self.network_momentum = args.momentum\n",
    "        self.network_weight_decay = args.weight_decay\n",
    "        self.model = model\n",
    "        self.arch_learning_rate = args.arch_learning_rate\n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.arch_learning_rate,\n",
    "                                                       beta1=0.5,\n",
    "                                                       beta2=0.999)    \n",
    "    def get_model_theta(self, model):\n",
    "        specific_tensor = []\n",
    "        specific_tensor_name = []\n",
    "        for var in model.trainable_weights:\n",
    "            if not 'alphas' in var.name:\n",
    "                specific_tensor.append(var)\n",
    "                specific_tensor_name.append(var.name)\n",
    "        return specific_tensor_name, specific_tensor\n",
    "    \n",
    "    def step(self, input_train, target_train, input_valid, target_valid, lr, unrolled):\n",
    "        \"\"\"Computer a step for gradient descend\n",
    "\n",
    "        Args:\n",
    "            input_train (tensor): a train of input\n",
    "            target_train (tensor): a train of targets\n",
    "            input_valid (tensor): a train of validation\n",
    "            target_valid (tensor): a train of validation targets\n",
    "            eta (tensor): eta\n",
    "            network_optimizer (optimiser): network optimiser for network\n",
    "            unrolled (bool): True if training we need unrolled\n",
    "        \"\"\"\n",
    "        train_loss = self.model._loss(self.model(input_train), target_train)\n",
    "        if unrolled:\n",
    "            self._compute_unrolled_step(\n",
    "                input_train, target_train, input_valid, target_valid, self.get_model_theta(self.model)[1], train_loss, lr)\n",
    "#         else:\n",
    "#             self._backward_step(input_valid, target_valid)\n",
    "        \n",
    "    def _compute_unrolled_step(self, x_train, y_train, x_valid, y_valid, w_var, train_loss, lr):\n",
    "        arch_var = self.model.arch_parameters()\n",
    "        unrolled_model = self.model.new()\n",
    "        unrolled_optimizer = tf.train.GradientDescentOptimizer(lr)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = unrolled_model(x_train)\n",
    "            unrolled_w_var = self.get_model_theta(unrolled_model)[1]\n",
    "            # copy weights\n",
    "            for v,w in zip(unrolled_w_var, w_var):\n",
    "                v.assign(w)\n",
    "            unrolled_train_loss = unrolled_model._criterion(logits, y_train)\n",
    "            grads = tape.gradient(unrolled_train_loss, unrolled_w_var)\n",
    "            unrolled_optimizer.apply_gradients(zip(grads, unrolled_w_var))\n",
    "        \n",
    "        with tf.control_dependencies([unrolled_optimizer]):\n",
    "            with tf.GradientTape() as tape1:\n",
    "                valid_loss = unrolled_model._criterion(unrolled_model(x_valid), y_valid)\n",
    "                valid_grads = tape1.gradient(valid_loss, unrolled_w_var)\n",
    "        \n",
    "        r=1e-2\n",
    "        R = r / tf.global_norm(valid_grads)\n",
    "        \n",
    "        print(valid_grads[0], w_var[0])\n",
    "        optimizer_pos=tf.train.GradientDescentOptimizer(R)\n",
    "        optimizer_pos=optimizer_pos.apply_gradients(zip(valid_grads, w_var))\n",
    "\n",
    "        optimizer_neg=tf.train.GradientDescentOptimizer(-2*R)\n",
    "        optimizer_neg=optimizer_neg.apply_gradients(zip(valid_grads, w_var))\n",
    "\n",
    "        optimizer_back=tf.train.GradientDescentOptimizer(R)\n",
    "        optimizer_back=optimizer_back.apply_gradients(zip(valid_grads, w_var))\n",
    "        \n",
    "        with tf.control_dependencies([optimizer_pos]):\n",
    "            with tf.GradientTape() as tape2:\n",
    "                train_grads_pos=tape2.gradient(train_loss, arch_var)\n",
    "            with tf.control_dependencies([optimizer_neg]):\n",
    "                with tf.GradientTape() as tape3:\n",
    "                    train_grads_neg=tape3.gradient(train_loss, arch_var)\n",
    "                with tf.control_dependencies([optimizer_back]):\n",
    "                    with tf.GradientTape() as tape4:\n",
    "                        leader_opt= self.optimizer\n",
    "                        leader_grads=tape4.gradient(valid_loss, arch_var)\n",
    "        \n",
    "        print(train_grads_pos)\n",
    "        for i,(g,v) in enumerate(zip(leader_grads, arch_var)):\n",
    "            leader_grads[i]=(g-lr*tf.divide(train_grads_pos[i]-train_grads_neg[i],2*R),v)\n",
    "\n",
    "        leader_opt=leader_opt.apply_gradients(leader_grads)\n",
    "        return leader_opt, unrolled_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dl)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
